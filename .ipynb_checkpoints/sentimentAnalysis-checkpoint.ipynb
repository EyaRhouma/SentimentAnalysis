{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis using a collection of nearly half a million Amazon reviews of mobiles phones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import entire libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#library for plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "#so that plots appear in the same browser window\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Import specific items only from the sklearn library\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from a file\n",
    "df0 = pd.read_csv(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.excel.ExcelFile'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ExcelFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8edbe14248b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#   containing one row per product and with columns = product attributes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ExcelFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Verify that this data consists of a matrix stored in a Pandas Data Frame\n",
    "#   containing one row per product and with columns = product attributes.\n",
    "print(type(df0))\n",
    "print(df0.shape)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 780 entries, 0 to 779\n",
      "Data columns (total 8 columns):\n",
      "Date                  776 non-null object\n",
      "Id                    776 non-null object\n",
      "Name                  776 non-null object\n",
      "Message               763 non-null object\n",
      "Like_Count            776 non-null float64\n",
      "Comment_Count         778 non-null float64\n",
      "Sentiment_Category    778 non-null float64\n",
      "url                   780 non-null object\n",
      "dtypes: float64(3), object(5)\n",
      "memory usage: 48.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#We can also use the info() method to obtain a summary of a pandas Data Frame\n",
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Message</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Sentiment_Category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>6 months ago</td>\n",
       "      <td>UghWNdfkaeJe33gCoAEC</td>\n",
       "      <td>mohamed chokou</td>\n",
       "      <td>Ma7laha spontané 3allé5r &lt;3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://youtu.be/-39SjIJ4324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>43102</td>\n",
       "      <td>3.285811643076E+14</td>\n",
       "      <td>Mamat Joury</td>\n",
       "      <td>ها بية حنانة هاك عاملة ماكياج</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii.Officiel....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>43102</td>\n",
       "      <td>3.2439688474104E+14</td>\n",
       "      <td>Ha Ma</td>\n",
       "      <td>عسل</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii.Officiel....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>43102</td>\n",
       "      <td>3.2385964144655E+14</td>\n",
       "      <td>Aymen Troudi</td>\n",
       "      <td>bou zinek</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>43102</td>\n",
       "      <td>1.02149130328024E+016</td>\n",
       "      <td>Sirine Lehyani</td>\n",
       "      <td>Mezyena</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                     Id            Name  \\\n",
       "779  6 months ago   UghWNdfkaeJe33gCoAEC  mohamed chokou   \n",
       "331         43102     3.285811643076E+14     Mamat Joury   \n",
       "477         43102    3.2439688474104E+14           Ha Ma   \n",
       "106         43102    3.2385964144655E+14    Aymen Troudi   \n",
       "94          43102  1.02149130328024E+016  Sirine Lehyani   \n",
       "\n",
       "                           Message  Like_Count  Comment_Count  \\\n",
       "779    Ma7laha spontané 3allé5r <3         9.0            0.0   \n",
       "331  ها بية حنانة هاك عاملة ماكياج         0.0            0.0   \n",
       "477                            عسل         0.0            0.0   \n",
       "106                      bou zinek         0.0            0.0   \n",
       "94                         Mezyena         0.0            0.0   \n",
       "\n",
       "     Sentiment_Category                                                url  \n",
       "779                 1.0                       https://youtu.be/-39SjIJ4324  \n",
       "331                 0.0  https://www.facebook.com/Baya.Zardii.Officiel....  \n",
       "477                 1.0  https://www.facebook.com/Baya.Zardii.Officiel....  \n",
       "106                 1.0  https://www.facebook.com/Baya.Zardii/photos/a....  \n",
       "94                  1.0  https://www.facebook.com/Baya.Zardii/photos/a....  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to speed up computation, we'll only use a small random sample\n",
    "#   of this data comprising 10 % of the rows.\n",
    "df = df0.sample(frac=0.1, random_state=2137)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763, 8)\n"
     ]
    }
   ],
   "source": [
    "# remove missing values by calling dropna() method\n",
    "#   arguments:  axis=0, how='any'  <=> remove all rows containing missing value in *any* column\n",
    "df0.dropna(inplace=True, axis=0, how='any')\n",
    "print(df0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "?df.dropna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "- a. Print number rows and columnsin the **original** matrix\n",
    "- b. Print number of rows and columns in the matrix **after** random sampling\n",
    "- c. print number of rows contain **missing** values in this matrix\n",
    "- d. Print data type of each column in the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and understand the data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of numeric attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Sentiment_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.36000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.138673</td>\n",
       "      <td>0.408469</td>\n",
       "      <td>0.83245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Like_Count  Comment_Count  Sentiment_Category\n",
       "count   75.000000      75.000000            75.00000\n",
       "mean     0.306667       0.093333             0.36000\n",
       "std      1.138673       0.408469             0.83245\n",
       "min      0.000000       0.000000            -1.00000\n",
       "25%      0.000000       0.000000             0.00000\n",
       "50%      0.000000       0.000000             1.00000\n",
       "75%      0.000000       0.000000             1.00000\n",
       "max      9.000000       3.000000             1.00000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The describe() method gives a numeric summary of numeric columns (attributes) only\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of brand names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(df0['Sentiment_Category']).most_common()\n",
    "\n",
    "#number of unique brand names\n",
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 376), (-1.0, 209), (0.0, 178)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequent brand names in the dataset\n",
    "counter[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of product names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "counter2 = Counter(df['Like_Count']).most_common()\n",
    "\n",
    "print(len(counter2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 51), (1.0, 8), (9.0, 1), (2.0, 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 Most frequent product names in the dataset\n",
    "counter2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of rating values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment_Category\n",
       "-1.0    209\n",
       " 0.0    178\n",
       " 1.0    376\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gR = df0.groupby('Sentiment_Category').size()\n",
    "print(type(gR))\n",
    "gR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 3 artists>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27095d0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(gR.index, gR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new binary ratings variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(585, 8)\n"
     ]
    }
   ],
   "source": [
    "# Remove any 'neutral' ratings equal to 3\n",
    "df0 = df0[df0['Sentiment_Category'] != 0]\n",
    "\n",
    "print(df0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Sentiment_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>585.000000</td>\n",
       "      <td>585.000000</td>\n",
       "      <td>585.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.304274</td>\n",
       "      <td>0.210256</td>\n",
       "      <td>0.285470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.167364</td>\n",
       "      <td>0.941016</td>\n",
       "      <td>0.959208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Like_Count  Comment_Count  Sentiment_Category\n",
       "count  585.000000     585.000000          585.000000\n",
       "mean     0.304274       0.210256            0.285470\n",
       "std      1.167364       0.941016            0.959208\n",
       "min      0.000000       0.000000           -1.000000\n",
       "25%      0.000000       0.000000           -1.000000\n",
       "50%      0.000000       0.000000            1.000000\n",
       "75%      0.000000       0.000000            1.000000\n",
       "max     13.000000      12.000000            1.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Message</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Sentiment_Category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>43102</td>\n",
       "      <td>1.4289770641202E+14</td>\n",
       "      <td>يونس بن سلهاب</td>\n",
       "      <td>عزوزة خارف تلم واعمل حل لعمرم</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii.Officiel....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>43102</td>\n",
       "      <td>1533175430053014</td>\n",
       "      <td>Samir Shabou</td>\n",
       "      <td>ما أتفهك....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii.Officiel....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>43102</td>\n",
       "      <td>5.2103964160613E+14</td>\n",
       "      <td>Massoudi Mounir</td>\n",
       "      <td>t3awef</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii.Officiel....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>43102</td>\n",
       "      <td>5.1277642576159E+14</td>\n",
       "      <td>يسرى أم أريج</td>\n",
       "      <td>Chbih fomha m3awej</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>43102</td>\n",
       "      <td>1.9755173693838E+15</td>\n",
       "      <td>Kacim Kaci</td>\n",
       "      <td>C pas vrai</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii.Officiel....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date                   Id             Name  \\\n",
       "329  43102  1.4289770641202E+14    يونس بن سلهاب   \n",
       "440  43102     1533175430053014     Samir Shabou   \n",
       "407  43102  5.2103964160613E+14  Massoudi Mounir   \n",
       "155  43102  5.1277642576159E+14     يسرى أم أريج   \n",
       "270  43102  1.9755173693838E+15       Kacim Kaci   \n",
       "\n",
       "                           Message  Like_Count  Comment_Count  \\\n",
       "329  عزوزة خارف تلم واعمل حل لعمرم         0.0            0.0   \n",
       "440                   ما أتفهك....         1.0            0.0   \n",
       "407                         t3awef         0.0            0.0   \n",
       "155             Chbih fomha m3awej         0.0            0.0   \n",
       "270                     C pas vrai         1.0            0.0   \n",
       "\n",
       "     Sentiment_Category                                                url  \n",
       "329                -1.0  https://www.facebook.com/Baya.Zardii.Officiel....  \n",
       "440                -1.0  https://www.facebook.com/Baya.Zardii.Officiel....  \n",
       "407                -1.0  https://www.facebook.com/Baya.Zardii.Officiel....  \n",
       "155                -1.0  https://www.facebook.com/Baya.Zardii/photos/a....  \n",
       "270                -1.0  https://www.facebook.com/Baya.Zardii.Officiel....  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some examples with high ratings\n",
    "df[df['Sentiment_Category'] == -1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "- a. Print products that have a \"neutral\" rating, i.e. a rating of 1\n",
    "- b. Print products that have a \"positive\" rating, i.e. rating of 4 or 5\n",
    "- c. Print products that have a \"negative\" rating, i.e. rating of 1 or 2\n",
    "- d. Print number of products of the brand name **Apple**  have a \"positive\" rating and how many have a \"negative\" rating   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "?train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df0['Message'], \n",
    "                                                    df0['Sentiment_Category'], \n",
    "                                                    random_state=591)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(438,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(438,)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(147,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47                   Belle femme\n",
       "238    Hhhhhhhhhhh mabheha da3wa\n",
       "7                          5ayba\n",
       "434               Ma7lek bayouta\n",
       "567             ki mameti 3zouza\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47     1.0\n",
       "238    1.0\n",
       "7     -1.0\n",
       "434    1.0\n",
       "567   -1.0\n",
       "Name: Sentiment_Category, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis based on BOW model with word occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Feature extraction* means representing raw text documents as numerical *feature vectors*.\n",
    "- In the simple BOW model, feature vector = number of word occurrences for each document and each vocabulary word.\n",
    "- We will do this using the ``CountVectorizer`` class: first we'll **tokenize** the documents and extract the vocabulary set, and then we determine the feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize documents & build vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "#  i.e. learn the vocabulary (distinct words) of the input corpus\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '100',\n",
       " '100000',\n",
       " '16',\n",
       " '170',\n",
       " '1961',\n",
       " '20',\n",
       " '2ans',\n",
       " '2ara2',\n",
       " '30',\n",
       " '3adeya',\n",
       " '3ahra',\n",
       " '3ala',\n",
       " '3alekher',\n",
       " '3alik',\n",
       " '3allé5r',\n",
       " '3am',\n",
       " '3amalit',\n",
       " '3amaliyet',\n",
       " '3amalyet',\n",
       " '3amla',\n",
       " '3amletlo',\n",
       " '3amlettou',\n",
       " '3amlthoum',\n",
       " '3and',\n",
       " '3andik',\n",
       " '3ar',\n",
       " '3asela',\n",
       " '3asla',\n",
       " '3assla',\n",
       " '3assoula',\n",
       " '3asssla',\n",
       " '3aych',\n",
       " '3az3ezti',\n",
       " '3bed',\n",
       " '3efcha',\n",
       " '3fata9',\n",
       " '3ifsha',\n",
       " '3ijbitni',\n",
       " '3in',\n",
       " '3inek',\n",
       " '3ini',\n",
       " '3inik',\n",
       " '3iyn',\n",
       " '3jbni',\n",
       " '3l5r',\n",
       " '3la',\n",
       " '3lch',\n",
       " '3leha',\n",
       " '3liha',\n",
       " '3liik',\n",
       " '3lik',\n",
       " '3lina',\n",
       " '3llikk',\n",
       " '3malt',\n",
       " '3morha',\n",
       " '3myen',\n",
       " '3omreha',\n",
       " '3omrek',\n",
       " '3omrha',\n",
       " '3omrk',\n",
       " '3sal',\n",
       " '3sayla',\n",
       " '3thertk',\n",
       " '3zoooooooouza',\n",
       " '3zouuuza',\n",
       " '3zouza',\n",
       " '3zza',\n",
       " '40',\n",
       " '4alina',\n",
       " '4och',\n",
       " '56',\n",
       " '57',\n",
       " '5aiba',\n",
       " '5ale9',\n",
       " '5arya',\n",
       " '5atrk',\n",
       " '5ayba',\n",
       " '5elti',\n",
       " '5fif',\n",
       " '5iiiiiiir',\n",
       " '5iiitt',\n",
       " '5iiîit',\n",
       " '5ir',\n",
       " '5irlk',\n",
       " '5it',\n",
       " '5kg',\n",
       " '5yout',\n",
       " '63',\n",
       " '65',\n",
       " '7a9',\n",
       " '7aja',\n",
       " '7ajja',\n",
       " '7ala',\n",
       " '7asdinek',\n",
       " '7atta',\n",
       " '7lili',\n",
       " '7loua',\n",
       " '7lowa',\n",
       " '7nekha',\n",
       " '7nina',\n",
       " '7ob',\n",
       " '7osed',\n",
       " '7othlet',\n",
       " '7wajbha',\n",
       " '7wajbk',\n",
       " '7wajeb',\n",
       " '8alta',\n",
       " '8atsa',\n",
       " '8irek',\n",
       " '9ad',\n",
       " '9adakch',\n",
       " '9adekch',\n",
       " '9albek',\n",
       " '9alet',\n",
       " '9alo',\n",
       " '9alou',\n",
       " '9asdeh',\n",
       " '9asdk',\n",
       " '9bal',\n",
       " '9dh',\n",
       " '9odm',\n",
       " '9ofa',\n",
       " '9ol',\n",
       " '9ololek',\n",
       " '9oul',\n",
       " 'a3leha',\n",
       " 'a3lem',\n",
       " 'a3mel',\n",
       " 'a3mell',\n",
       " 'a3rf',\n",
       " 'a5tini',\n",
       " 'a7lowa',\n",
       " 'aaaaa',\n",
       " 'aaaazouza',\n",
       " 'aba3tile',\n",
       " 'adore',\n",
       " 'ah',\n",
       " 'ahla',\n",
       " 'ak',\n",
       " 'akther',\n",
       " 'ala',\n",
       " 'alah',\n",
       " 'alger',\n",
       " 'alghalb',\n",
       " 'ali',\n",
       " 'aliha',\n",
       " 'alik',\n",
       " 'allah',\n",
       " 'always',\n",
       " 'ama',\n",
       " 'aman',\n",
       " 'amel',\n",
       " 'amina',\n",
       " 'amoura',\n",
       " 'amrouche',\n",
       " 'amset',\n",
       " 'amtelk',\n",
       " 'ana',\n",
       " 'ans',\n",
       " 'aslha',\n",
       " 'avec',\n",
       " 'awil',\n",
       " 'aya',\n",
       " 'ayab',\n",
       " 'azeyane',\n",
       " 'azouza',\n",
       " 'azyan',\n",
       " 'azyeen',\n",
       " 'azyen',\n",
       " 'azzyin',\n",
       " 'b1000000000',\n",
       " 'ba3thek',\n",
       " 'bagra',\n",
       " 'bahbouha',\n",
       " 'bahom',\n",
       " 'bara',\n",
       " 'barcha',\n",
       " 'barii',\n",
       " 'bariiii',\n",
       " 'barka',\n",
       " 'bassem',\n",
       " 'bay',\n",
       " 'baya',\n",
       " 'bayna',\n",
       " 'bayota',\n",
       " 'bayouna',\n",
       " 'bayouta',\n",
       " 'bayoutaa',\n",
       " 'bayoutaaaa',\n",
       " 'bayouutta',\n",
       " 'bayya',\n",
       " 'bayyouta',\n",
       " 'bb',\n",
       " 'beauty',\n",
       " 'beauté',\n",
       " 'bech',\n",
       " 'beel',\n",
       " 'behia',\n",
       " 'bel',\n",
       " 'bel7a9',\n",
       " 'belal',\n",
       " 'bell',\n",
       " 'bella',\n",
       " 'belle',\n",
       " 'bellessima',\n",
       " 'bello',\n",
       " 'ben',\n",
       " 'benesba',\n",
       " 'benti',\n",
       " 'berah',\n",
       " 'berge',\n",
       " 'beya',\n",
       " 'beyen',\n",
       " 'beyouta',\n",
       " 'bhar',\n",
       " 'bhima',\n",
       " 'bien',\n",
       " 'bih',\n",
       " 'bihom',\n",
       " 'bil',\n",
       " 'bilhak',\n",
       " 'bkolha',\n",
       " 'bla',\n",
       " 'blach',\n",
       " 'blech',\n",
       " 'blmahleek',\n",
       " 'bnaya',\n",
       " 'bnt',\n",
       " 'bnty',\n",
       " 'boba',\n",
       " 'bon',\n",
       " 'botox',\n",
       " 'bou',\n",
       " 'boumerdes',\n",
       " 'brasmi',\n",
       " 'bravo',\n",
       " 'bravoooooooooooooooo',\n",
       " 'briyon',\n",
       " 'brou7ek',\n",
       " 'bsara7a',\n",
       " 'bsaraha',\n",
       " 'byottttta',\n",
       " 'byouta',\n",
       " 'cava',\n",
       " 'ch3irkk',\n",
       " 'cha',\n",
       " 'cha3erk',\n",
       " 'cha5sitha',\n",
       " 'chab3a',\n",
       " 'chabeb',\n",
       " 'chadit',\n",
       " 'chai',\n",
       " 'chanti',\n",
       " 'charmantes',\n",
       " 'charmonte',\n",
       " 'chay',\n",
       " 'che7ra9',\n",
       " 'cheda',\n",
       " 'chedde',\n",
       " 'chibb',\n",
       " 'chkoun',\n",
       " 'chleka',\n",
       " 'chneya',\n",
       " 'chnwa',\n",
       " 'chofetha',\n",
       " 'choftek',\n",
       " 'chwaya',\n",
       " 'chyb',\n",
       " 'claaaaaaas',\n",
       " 'clair',\n",
       " 'classe',\n",
       " 'colèration',\n",
       " 'coureg',\n",
       " 'cute',\n",
       " 'da3wa',\n",
       " 'dana',\n",
       " 'dari',\n",
       " 'dawem',\n",
       " 'dbachha',\n",
       " 'de',\n",
       " 'deco',\n",
       " 'deleni',\n",
       " 'denia',\n",
       " 'dhaher',\n",
       " 'dhahra',\n",
       " 'dhou9',\n",
       " 'dhou9ik',\n",
       " 'dima',\n",
       " 'din',\n",
       " 'doute',\n",
       " 'dr1',\n",
       " 'dz',\n",
       " 'dénia',\n",
       " 'e7tirami',\n",
       " 'ebyoutna',\n",
       " 'el',\n",
       " 'elegante',\n",
       " 'eli',\n",
       " 'elkol',\n",
       " 'elle',\n",
       " 'elli',\n",
       " 'elmezyen',\n",
       " 'elte3ml',\n",
       " 'ely',\n",
       " 'ema',\n",
       " 'emmchii',\n",
       " 'emmm',\n",
       " 'en',\n",
       " 'ena',\n",
       " 'encore',\n",
       " 'enhbha',\n",
       " 'enjasttouna',\n",
       " 'enrobé',\n",
       " 'enti',\n",
       " 'entouma',\n",
       " 'enty',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'etfate9',\n",
       " 'ey',\n",
       " 'eyy',\n",
       " 'f3lk',\n",
       " 'faha',\n",
       " 'fak',\n",
       " 'fama',\n",
       " 'far5aa',\n",
       " 'far7an',\n",
       " 'fawzi',\n",
       " 'fe',\n",
       " 'feha',\n",
       " 'fehri',\n",
       " 'femme',\n",
       " 'femmme',\n",
       " 'fenia',\n",
       " 'fhemtk',\n",
       " 'fi',\n",
       " 'fih',\n",
       " 'fiha',\n",
       " 'fihh',\n",
       " 'fik',\n",
       " 'fil',\n",
       " 'fin',\n",
       " 'fina',\n",
       " 'fièrent',\n",
       " 'fom',\n",
       " 'fond',\n",
       " 'fort',\n",
       " 'foumk',\n",
       " 'good',\n",
       " 'grave',\n",
       " 'ha',\n",
       " 'haadhika',\n",
       " 'haguar',\n",
       " 'hahahahaha',\n",
       " 'hailayt',\n",
       " 'haja',\n",
       " 'hajja',\n",
       " 'hak',\n",
       " 'haka',\n",
       " 'hal',\n",
       " 'hala',\n",
       " 'harri99',\n",
       " 'hata',\n",
       " 'hathi',\n",
       " 'hawinek',\n",
       " 'haylaaa',\n",
       " 'hbel',\n",
       " 'hedhoukem',\n",
       " 'hedi',\n",
       " 'heja',\n",
       " 'hek',\n",
       " 'heka',\n",
       " 'heki',\n",
       " 'hetha',\n",
       " 'hethi',\n",
       " 'hh',\n",
       " 'hhh',\n",
       " 'hhhh',\n",
       " 'hhhhh',\n",
       " 'hhhhhh',\n",
       " 'hhhhhhh',\n",
       " 'hhhhhhhhh',\n",
       " 'hhhhhhhhhh',\n",
       " 'hhhhhhhhhhh',\n",
       " 'hhhhhhhhhhhhhhhhh',\n",
       " 'hia',\n",
       " 'hindem',\n",
       " 'hiya',\n",
       " 'hlewa',\n",
       " 'hlouwa',\n",
       " 'hob',\n",
       " 'houni',\n",
       " 'houwa',\n",
       " 'howa',\n",
       " 'héki',\n",
       " 'i7chem',\n",
       " 'i7chm',\n",
       " 'idraz',\n",
       " 'ih',\n",
       " 'il',\n",
       " 'ili',\n",
       " 'ill',\n",
       " 'ilmou5if',\n",
       " 'inte',\n",
       " 'inti',\n",
       " 'intiii',\n",
       " 'inégalé',\n",
       " 'is7i7',\n",
       " 'itwada3',\n",
       " 'itwansa',\n",
       " 'jamehir',\n",
       " 'jarna',\n",
       " 'jidiya',\n",
       " 'jidya',\n",
       " 'jloudha',\n",
       " 'jmai',\n",
       " 'jolie',\n",
       " 'jolies',\n",
       " 'jwayed',\n",
       " 'ka7a',\n",
       " 'kalb',\n",
       " 'kan',\n",
       " 'kardeşim',\n",
       " 'karim',\n",
       " 'kathaba',\n",
       " 'kbert',\n",
       " 'kbira',\n",
       " 'kdeba',\n",
       " 'kebret',\n",
       " 'kelmtyn',\n",
       " 'ken',\n",
       " 'khali',\n",
       " 'khaterha',\n",
       " 'khawla',\n",
       " 'khawte',\n",
       " 'khayba',\n",
       " 'ki',\n",
       " 'kif',\n",
       " 'kifech',\n",
       " 'kil',\n",
       " 'kima',\n",
       " 'klba',\n",
       " 'kn',\n",
       " 'kol',\n",
       " 'koul',\n",
       " 'kız',\n",
       " 'l3ada',\n",
       " 'l3iiib',\n",
       " 'l3yb',\n",
       " 'l7ayet',\n",
       " 'l9ird',\n",
       " 'la',\n",
       " 'la3bed',\n",
       " 'lachkel',\n",
       " 'laha',\n",
       " 'layla',\n",
       " 'le3b',\n",
       " 'lebhim',\n",
       " 'lebsa',\n",
       " 'lel',\n",
       " 'lezm',\n",
       " 'li',\n",
       " 'lik',\n",
       " 'likoum',\n",
       " 'ljazair',\n",
       " 'lkithb',\n",
       " 'lkol',\n",
       " 'lmitkabir',\n",
       " 'lmot7f',\n",
       " 'lmra',\n",
       " 'lokhrin',\n",
       " 'look',\n",
       " 'lotf',\n",
       " 'love',\n",
       " 'lrabi',\n",
       " 'm3a',\n",
       " 'm3aha',\n",
       " 'm3ak',\n",
       " 'm3arsa',\n",
       " 'm3nha',\n",
       " 'm5ybk',\n",
       " 'm7alk',\n",
       " 'm7lllllak',\n",
       " 'ma',\n",
       " 'ma2arwa3ak',\n",
       " 'ma3adech',\n",
       " 'ma3andhomech',\n",
       " 'ma3jbo',\n",
       " 'ma3raftekch',\n",
       " 'ma5ibik',\n",
       " 'ma5lou9a',\n",
       " 'ma5ybik',\n",
       " 'ma5yeb',\n",
       " 'ma7boubat',\n",
       " 'ma7la',\n",
       " 'ma7laha',\n",
       " 'ma7lak',\n",
       " 'ma7leha',\n",
       " 'ma7lehak',\n",
       " 'ma7lek',\n",
       " 'ma7lekk',\n",
       " 'ma7lik',\n",
       " 'ma7lk',\n",
       " 'ma8iir',\n",
       " 'mabheha',\n",
       " 'machalah',\n",
       " 'machalh',\n",
       " 'machalla',\n",
       " 'machallah',\n",
       " 'macyaj',\n",
       " 'mafama',\n",
       " 'mafameche',\n",
       " 'maguir',\n",
       " 'mahlaik',\n",
       " 'mahlak',\n",
       " 'mahleek',\n",
       " 'mahleha',\n",
       " 'mahlek',\n",
       " 'mahlekk',\n",
       " 'mahllek',\n",
       " 'mahyech',\n",
       " 'mais',\n",
       " 'mak',\n",
       " 'make',\n",
       " 'makiage',\n",
       " 'makija',\n",
       " 'makillage',\n",
       " 'makup',\n",
       " 'makyage',\n",
       " 'makyaje',\n",
       " 'makyajek',\n",
       " 'mal9it',\n",
       " 'mal9itt',\n",
       " 'mala',\n",
       " 'maman',\n",
       " 'mameti',\n",
       " 'mamety',\n",
       " 'man77ebikish',\n",
       " 'man7ebech',\n",
       " 'man9oul',\n",
       " 'mandher',\n",
       " 'mandhir',\n",
       " 'mani',\n",
       " 'manther',\n",
       " 'maq',\n",
       " 'maqeillege',\n",
       " 'maqiage',\n",
       " 'maquiallage',\n",
       " 'maquiallge',\n",
       " 'maquillage',\n",
       " 'maquillages',\n",
       " 'mara',\n",
       " 'marabetek',\n",
       " 'mariem',\n",
       " 'marra',\n",
       " 'masali',\n",
       " 'mascara',\n",
       " 'masta',\n",
       " 'mastaaaaaaaaaaaaa',\n",
       " 'mastoura',\n",
       " 'mat7a9etlich',\n",
       " 'mata7chemech',\n",
       " 'matafla7',\n",
       " 'matasla7',\n",
       " 'mati3jibnich',\n",
       " 'mauve',\n",
       " 'maydoumlk',\n",
       " 'maye3nouli',\n",
       " 'mazelt',\n",
       " 'mazinek',\n",
       " 'mchit',\n",
       " 'mdr',\n",
       " 'mdrrrrrr',\n",
       " 'me',\n",
       " 'me3arestch',\n",
       " 'mech',\n",
       " 'meghiara',\n",
       " 'mel',\n",
       " 'mela',\n",
       " 'melehbel',\n",
       " 'memi',\n",
       " 'men',\n",
       " 'men8ir',\n",
       " 'menha',\n",
       " 'meni',\n",
       " 'menn',\n",
       " 'merci',\n",
       " 'meteswe',\n",
       " 'metetmena',\n",
       " 'meziaaaaaana',\n",
       " 'meziana',\n",
       " 'meziena',\n",
       " 'mezyena',\n",
       " 'mezzzzziena',\n",
       " 'mgalgetni',\n",
       " 'mhlek',\n",
       " 'mi',\n",
       " 'miboun',\n",
       " 'michay',\n",
       " 'migale',\n",
       " 'min',\n",
       " 'mine',\n",
       " 'minek',\n",
       " 'minha',\n",
       " 'minik',\n",
       " 'mit',\n",
       " 'mitkabra',\n",
       " 'mizyana',\n",
       " 'mjalda',\n",
       " 'mkch',\n",
       " 'mlakez',\n",
       " 'mmmm',\n",
       " 'mmmma7lek',\n",
       " 'mn',\n",
       " 'mnawra',\n",
       " 'mnawrin',\n",
       " 'mo5i',\n",
       " 'moch',\n",
       " 'mohamed',\n",
       " 'mohsenn',\n",
       " 'mojttama3',\n",
       " 'morsi',\n",
       " 'mouch',\n",
       " 'mouche',\n",
       " 'mouchek',\n",
       " 'mounachta',\n",
       " 'mouthi3',\n",
       " 'mr6',\n",
       " 'mrama',\n",
       " 'mrc',\n",
       " 'msam7a',\n",
       " 'msikna',\n",
       " 'mta3',\n",
       " 'mthabta',\n",
       " 'mtswa',\n",
       " 'mtt7rkch',\n",
       " 'mwahra',\n",
       " 'mwhra',\n",
       " 'mzyena',\n",
       " 'mzyna',\n",
       " 'mêla',\n",
       " 'même',\n",
       " 'n3ref',\n",
       " 'n7bk',\n",
       " 'n7bok',\n",
       " 'n9olha',\n",
       " 'na7i',\n",
       " 'na7it',\n",
       " 'na7na',\n",
       " 'nabi',\n",
       " 'nahdhr',\n",
       " 'nahkouch',\n",
       " 'nakirhiik',\n",
       " 'nakirhik',\n",
       " 'nakraha',\n",
       " 'nakrehek',\n",
       " 'nbkhro',\n",
       " 'nchala',\n",
       " 'nchouf',\n",
       " 'nchoufouk',\n",
       " 'ne',\n",
       " 'nertehou',\n",
       " 'nes',\n",
       " 'nesre9',\n",
       " 'netttttttna',\n",
       " 'nhabkom',\n",
       " 'nhar',\n",
       " 'nhb',\n",
       " 'nhebek',\n",
       " 'nhebha',\n",
       " 'nhebk',\n",
       " 'nheebha',\n",
       " 'nhibek',\n",
       " 'nice',\n",
       " 'nil',\n",
       " 'nji',\n",
       " 'nkrha',\n",
       " 'nmout',\n",
       " 'nn',\n",
       " 'non',\n",
       " 'normale',\n",
       " 'noufel',\n",
       " 'nte9',\n",
       " 'nude',\n",
       " 'ok',\n",
       " 'okhti',\n",
       " 'omezyena',\n",
       " 'omi',\n",
       " 'on',\n",
       " 'osra',\n",
       " 'ou',\n",
       " 'oui',\n",
       " 'oumor',\n",
       " 'oumour',\n",
       " 'ouuhh',\n",
       " 'panso',\n",
       " 'pas',\n",
       " 'pauvre',\n",
       " 'pinceau',\n",
       " 'plus',\n",
       " 'quand',\n",
       " 'ra9tek',\n",
       " 'rabbi',\n",
       " 'rabetek',\n",
       " 'rabi',\n",
       " 'raby',\n",
       " 'rahi',\n",
       " 'rak',\n",
       " 'rak3a',\n",
       " 'ram',\n",
       " 'rani',\n",
       " 'rasmethoum',\n",
       " 'ravissante',\n",
       " 'raw',\n",
       " 'rawdha',\n",
       " 'raww',\n",
       " 'regarde',\n",
       " 'retraite',\n",
       " 'richa',\n",
       " 'rjou3',\n",
       " 'rou7aa',\n",
       " 'rou7k',\n",
       " 'rou7ék',\n",
       " 'rouge',\n",
       " 'rouhek',\n",
       " 'sadek',\n",
       " 'sadka',\n",
       " 'sajda',\n",
       " 'sajeda',\n",
       " 'salam',\n",
       " 'salami',\n",
       " 'salit',\n",
       " 'sami',\n",
       " 'sans',\n",
       " 'saya',\n",
       " 'saybb',\n",
       " 'sayeb',\n",
       " 'saïd',\n",
       " 'se',\n",
       " 'seb7a',\n",
       " 'sebha',\n",
       " 'seni',\n",
       " 'sensible',\n",
       " 'serwelk',\n",
       " 'seviyorum',\n",
       " 'sfiiha',\n",
       " 'shay',\n",
       " 'shih',\n",
       " 'shnwa',\n",
       " 'si',\n",
       " 'simple',\n",
       " 'simplicité',\n",
       " 'sne',\n",
       " 'snin',\n",
       " 'sob7an',\n",
       " 'sob7nou',\n",
       " 'sobhan',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'soussa',\n",
       " 'spontané',\n",
       " 'spontanée',\n",
       " 'srrt',\n",
       " 'suuper',\n",
       " 'sémm7a',\n",
       " 't3awef',\n",
       " 't3jebni',\n",
       " 't7eb',\n",
       " 't7ibo',\n",
       " 't9olhom',\n",
       " 't9oul',\n",
       " 'ta3tou',\n",
       " 'ta4ou',\n",
       " 'ta7founa',\n",
       " 'tab9a',\n",
       " 'tabdi',\n",
       " 'tabrkla',\n",
       " 'tahiyate',\n",
       " 'tais',\n",
       " 'tajmil',\n",
       " 'tamou7a',\n",
       " 'tanja7',\n",
       " 'taraji',\n",
       " 'taswira',\n",
       " 'tata',\n",
       " 'taw',\n",
       " 'tawa',\n",
       " 'tawaaa',\n",
       " 'tawba',\n",
       " 'tay7a',\n",
       " 'tayara',\n",
       " 'tb9',\n",
       " 'tb9i',\n",
       " 'tbarkala',\n",
       " 'tbarkalah',\n",
       " 'tbarkalla',\n",
       " 'tbarkallah',\n",
       " 'tchabah',\n",
       " 'tchbah',\n",
       " 'tched',\n",
       " 'te3jebni',\n",
       " 'te7bek',\n",
       " 'te7chem',\n",
       " 'techrab',\n",
       " 'tekdeb',\n",
       " 'tekdheb',\n",
       " 'tesghar',\n",
       " 'tetghchech',\n",
       " 'tetsana3',\n",
       " 'tfata9',\n",
       " 'tfete9',\n",
       " 'tfj3iiiiiiiiiiiii',\n",
       " 'tgoli',\n",
       " 'tha9afa',\n",
       " 'thaballl',\n",
       " 'thabbil',\n",
       " 'thabel',\n",
       " 'thabele',\n",
       " 'thabil',\n",
       " 'thechi',\n",
       " 'thnitek',\n",
       " 'ti',\n",
       " 'tii',\n",
       " 'tijri',\n",
       " 'tir',\n",
       " 'tirr',\n",
       " 'tirran',\n",
       " 'tji',\n",
       " 'tjmil',\n",
       " 'tjr',\n",
       " 'tkark',\n",
       " 'tkoli',\n",
       " 'tmanitha',\n",
       " 'to9tel',\n",
       " 'to9tell',\n",
       " 'tod5ol',\n",
       " 'todhhor',\n",
       " 'toi',\n",
       " 'tok3ed',\n",
       " 'toooooooooop',\n",
       " 'tooooop',\n",
       " 'toooppp',\n",
       " 'top',\n",
       " 'toppp',\n",
       " 'torbya',\n",
       " 'tou9tel',\n",
       " 'toub',\n",
       " 'toujours',\n",
       " 'toumou7',\n",
       " 'tounes',\n",
       " 'touns',\n",
       " 'tres',\n",
       " 'trop',\n",
       " 'très',\n",
       " 'tskhayel',\n",
       " 'tt9o7b',\n",
       " 'ttsanna3',\n",
       " 'tu',\n",
       " 'tw',\n",
       " 'twa',\n",
       " 'ty',\n",
       " 'tz3bin',\n",
       " 'tzakem',\n",
       " 'tzid',\n",
       " 'ui',\n",
       " 'uii',\n",
       " 'une',\n",
       " 'up',\n",
       " 'vert',\n",
       " 'very',\n",
       " 'voilà',\n",
       " 'vousetestresbelle',\n",
       " 'vrai',\n",
       " 'vraiment',\n",
       " 'w3andik',\n",
       " 'w5doudek',\n",
       " 'w9ayet',\n",
       " 'w9ayett',\n",
       " 'wa',\n",
       " 'wa7ch',\n",
       " 'wahad',\n",
       " 'wahra',\n",
       " 'wal7i',\n",
       " 'wala',\n",
       " 'walah',\n",
       " 'walahi',\n",
       " 'walet',\n",
       " 'walh',\n",
       " 'wallah',\n",
       " 'wallh',\n",
       " 'wassel',\n",
       " 'waw',\n",
       " 'wbbarra',\n",
       " 'we',\n",
       " 'we7la',\n",
       " 'we9tech',\n",
       " 'wejha',\n",
       " 'wejhek',\n",
       " 'wejhk',\n",
       " 'wela',\n",
       " 'weli',\n",
       " 'whla3ba',\n",
       " 'wi9tha',\n",
       " 'wili',\n",
       " 'willayat',\n",
       " 'win',\n",
       " 'wklo8a',\n",
       " 'wlh',\n",
       " 'wma7la',\n",
       " 'wmohtarma',\n",
       " 'wodhnii',\n",
       " 'womniti',\n",
       " 'woooh',\n",
       " 'woooo',\n",
       " 'wor9od',\n",
       " 'wrabi',\n",
       " 'wrzina',\n",
       " 'wtsyb',\n",
       " 'y3awed',\n",
       " 'y3ayet',\n",
       " 'y5alini',\n",
       " 'y9olk',\n",
       " 'ya',\n",
       " 'ya3tek',\n",
       " 'ya3tik',\n",
       " 'ya7mik',\n",
       " 'yahafdhak',\n",
       " 'yahhafdhak',\n",
       " 'yakhi',\n",
       " 'yaser',\n",
       " 'ye',\n",
       " 'yena3en',\n",
       " 'yeser',\n",
       " 'yetsawrouh',\n",
       " 'yezi',\n",
       " 'yheloulha',\n",
       " 'yjiboulha',\n",
       " 'ykarker',\n",
       " 'ykeberne',\n",
       " 'ym3lm',\n",
       " 'you',\n",
       " 'youki',\n",
       " 'ytfach',\n",
       " 'ytsal',\n",
       " 'ywaf9ak',\n",
       " 'ywafkik',\n",
       " 'yzi',\n",
       " 'za3ma',\n",
       " 'zaid',\n",
       " 'zardi',\n",
       " 'zayn',\n",
       " 'zeda',\n",
       " 'zid',\n",
       " 'zin',\n",
       " 'zinek',\n",
       " 'zinha',\n",
       " 'zinik',\n",
       " 'zink',\n",
       " 'zinkkk',\n",
       " 'zitou',\n",
       " 'ziyn',\n",
       " 'é7taram',\n",
       " 'émission',\n",
       " 'آش',\n",
       " 'أشباه',\n",
       " 'أشرف',\n",
       " 'أمي',\n",
       " 'أنا',\n",
       " 'أنيقة',\n",
       " 'إبعدي',\n",
       " 'إنتي',\n",
       " 'اءبتسا',\n",
       " 'اءيتها',\n",
       " 'ابن',\n",
       " 'اتقوا',\n",
       " 'احسن',\n",
       " 'احلا',\n",
       " 'احلي',\n",
       " 'اختي',\n",
       " 'اروح',\n",
       " 'اعليها',\n",
       " 'اقل',\n",
       " 'اكبر',\n",
       " 'اكييييد',\n",
       " 'الاحد',\n",
       " 'الاخلاق',\n",
       " 'الاخير',\n",
       " 'الاوقات',\n",
       " 'البلادة',\n",
       " 'التربية',\n",
       " 'التفاهة',\n",
       " 'التماسيح',\n",
       " 'التونسي',\n",
       " 'الجزائر',\n",
       " 'الجمال',\n",
       " 'الحصان',\n",
       " 'الخايبة',\n",
       " 'الخضراء',\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print words are there in the vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1445"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct document-term matrix\n",
    "- This matrix contains the *feature vectors* of a given set of raw documents.\n",
    "- For the simple BOW model, feature vector = number of word occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(438, 1445)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(147, 1445)\n"
     ]
    }
   ],
   "source": [
    "# the document-term matrix for the training corpus\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(type(X_train_vectorized))\n",
    "print(X_train_vectorized.shape)\n",
    "\n",
    "# the document-term matrix for the test corpus\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "print(type(X_test_vectorized))\n",
    "print(X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2119\n"
     ]
    }
   ],
   "source": [
    "# Number of non-zero elements in document-term matrix of training corpus\n",
    "print(X_train_vectorized.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the proportion of non-zero elements in ``X_train_vectorized`` document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033480273656602043"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.nnz / (X_train_vectorized.shape[0] * X_train_vectorized.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1445)\n",
      "1\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Number of training documents that contain each word (called document frequency) fema kelma wawjouda fi document bark w fema kelma mawjouda fi 24 doc\n",
    "doc_freq = np.array((X_train_vectorized > 0).sum(0))\n",
    "print(doc_freq.shape)\n",
    "print(np.amin(doc_freq))\n",
    "print(np.amax(doc_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1445)\n",
      "0.00228310502283\n",
      "0.0547945205479\n"
     ]
    }
   ],
   "source": [
    "# Proportion of training documents that contain each word (called relative document frequency)\n",
    "n,m = X_train_vectorized.shape\n",
    "rel_doc_freq = np.array((X_train_vectorized > 0).sum(0)/n)\n",
    "\n",
    "print(rel_doc_freq.shape)\n",
    "print(np.amin(rel_doc_freq))\n",
    "print(np.amax(rel_doc_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 1)\n",
      "0\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# Number of unique words in each training document/ fema doc fih 0 kemla w fema doc fih 38 kemla\n",
    "words_per_doc = np.array((X_train_vectorized > 0).sum(1))\n",
    "print(words_per_doc.shape)\n",
    "print(np.amin(words_per_doc))\n",
    "print(np.amax(words_per_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a. Print number of training documents contain 0 words\n",
    "- b. Print number of training documents contain 1 unique word\n",
    "- c. Print number of training documents contain 2 unique words\n",
    "\n",
    "We can either use ``np.sum`` method with the variable ``words_per_doc`` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_per_doc[words_per_doc ==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.12290000e+04,   1.39000000e+03,   2.69000000e+02,\n",
       "          8.60000000e+01,   3.50000000e+01,   1.50000000e+01,\n",
       "          8.00000000e+00,   1.00000000e+00,   6.00000000e+00,\n",
       "          1.00000000e+00]),\n",
       " array([   0. ,   74.6,  149.2,  223.8,  298.4,  373. ,  447.6,  522.2,\n",
       "         596.8,  671.4,  746. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feedee4f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of number of unique words in each document\n",
    "plt.hist(words_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of number of document frequency of words\n",
    "# plt.hist(doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove documents with 0 words from training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411          ❤💖\n",
       "555    J.t.m ❤❤\n",
       "356         😌😖😖\n",
       "642          ❤❤\n",
       "531          <3\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select documents that contain 0 words\n",
    "idx = np.where(words_per_doc == 0)[0]\n",
    "# Show all those documents\n",
    "X_train.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433,)\n"
     ]
    }
   ],
   "source": [
    "## Remove rows from training data that contain no words\n",
    "X_train = X_train.drop(X_train.index[idx])\n",
    "y_train = y_train.drop(y_train.index[idx])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 1445)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-compute the document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433, 1)\n",
      "1\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# verify results: re-compute number of words in each document\n",
    "words_per_doc = np.array((X_train_vectorized > 0).sum(1))\n",
    "print(words_per_doc.shape)\n",
    "print(np.amin(words_per_doc))\n",
    "print(np.amax(words_per_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classification model using Logistic Regression\n",
    "We are going to  to build a classification model using the feature vectors of the training documents (which are stored in the variable ``X_train_vectorized``) and their corresponding true sentiment categories (which are stored in the variable ``y_train``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using LR method\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the classification model\n",
    "We'll use the obtained LR model to predict sentiment categories (classes) of test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(147,)\n"
     ]
    }
   ],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "LR_predictions = LR_model.predict(X_test_vectorized)\n",
    "print(type(LR_predictions))\n",
    "print(LR_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.]\n",
      "[-1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(LR_predictions[:30])\n",
    "print(np.array(y_test[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the first 30 test documents, how many predictions are wrong?  *Hint*: see the output of the above cell.\n",
    "- For the first 1000 test documents, how many predictions are wrong?  *Hint*: use an expression of the form: ``np.sum(a == b)`` where a and b are two arrays of the same size.\n",
    "- What is the *classification rate* for the entire test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(147, 2)\n",
      "[[ 0.46485078  0.53514922]\n",
      " [ 0.16431827  0.83568173]\n",
      " [ 0.18042432  0.81957568]\n",
      " [ 0.27803192  0.72196808]\n",
      " [ 0.82038889  0.17961111]\n",
      " [ 0.71969634  0.28030366]\n",
      " [ 0.07019447  0.92980553]\n",
      " [ 0.30157635  0.69842365]\n",
      " [ 0.21246429  0.78753571]\n",
      " [ 0.32725931  0.67274069]]\n"
     ]
    }
   ],
   "source": [
    "# Actually, we can also obtain the prediction probabilities for each sentiment category\n",
    "LR_pred_prob = LR_model.predict_proba(X_test_vectorized)\n",
    "print(type(LR_pred_prob))\n",
    "print(LR_pred_prob.shape)\n",
    "\n",
    "# the prediction probabilities for the first 10 test documents\n",
    "print(LR_pred_prob[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance of classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78231292517006801"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate model's classification rate on the test corpus\n",
    "\n",
    "LR_classif_rate = accuracy_score(y_test, LR_predictions)\n",
    "LR_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Interpretation of model's coefficients (parameters)\n",
    "Print which vocabulary words are most important in our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1445,)\n"
     ]
    }
   ],
   "source": [
    "# first get LR model's coefficient (there is one coefficient per vocabulary word)\n",
    "coefs = LR_model.coef_[0]\n",
    "print(coefs.shape)\n",
    "\n",
    "# Sort these coefficient values in ascending order\n",
    "sorted_coef_index = coefs.argsort()  #sort by actual value\n",
    "sorted_coef_index_2 = abs(coefs).argsort()  #sort by absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs of LR model:\n",
      "\n",
      " Coefficient values:\n",
      "[-1.66356296 -1.62225352 -1.61714436 -1.28702415 -1.10875703 -0.99170879\n",
      " -0.90704645 -0.79517843 -0.77632431 -0.75710403]\n",
      "\n",
      " Feature names:\n",
      "['3zouza' '5ayba' 'عزوزة' 'masta' 'maquillage' 'في' 't3awef' 'hhhhhh' 'ya'\n",
      " '3ifsha']\n",
      "\n",
      "Largest Coefs of LR model:\n",
      "\n",
      " Coefficient values: \n",
      "[ 1.53688888  1.34706152  1.07814788  0.98554512  0.92528683  0.90997876\n",
      "  0.903053    0.83388748  0.78340395  0.76051351]\n",
      " Feature names: \n",
      "['belle' 'bayouta' 'ma7lek' 'mahlek' 'محلاك' '3asla' 'thabel' 'dima'\n",
      " 'ma7leha' '3lik']\n",
      "Smallest abs(Coefs):\n",
      "\n",
      " Coefficient values:\n",
      "[ 0.00128754  0.0021675  -0.00221346  0.00270874  0.00270874  0.00270874\n",
      "  0.00270874  0.00270874  0.00270874  0.00270874]\n",
      "\n",
      " Feature names:\n",
      "['العمر' 'benti' 'hhhhhhhhhhhhhhhhh' 'mohamed' 'ljazair' 'salami'\n",
      " 'boumerdes' 'jidya' 'yahafdhak' 'bay']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the 10 smallest and 10 largest coefficients\n",
    "\n",
    "#feature_names = vect.get_feature_names()\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "print('Smallest Coefs of LR model:\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "\n",
    "print('Largest Coefs of LR model:\\n')\n",
    "print(' Coefficient values: \\n{}'.format(coefs[sorted_coef_index[:-11:-1]]))\n",
    "print(' Feature names: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "\n",
    "print('Smallest abs(Coefs):\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index_2[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index_2[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classification model using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Naive Bayes classification model\n",
    "\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this model to predict sentiment of test documents\n",
    "\n",
    "NB_predictions = NB_model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78911564625850339"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate model's classification rate on the test corpus\n",
    "\n",
    "NB_classif_rate = accuracy_score(y_test, NB_predictions)\n",
    "NB_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using BOW model and Tfidf\n",
    "We are basically going to re-do the same steps as above, but using the ``TfidfVectorizer`` class instead of ``CountVectorizer`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build vocabulary\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "tfidf_vect = TfidfVectorizer(min_df=5)\n",
    "tfidf_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we obtained a smaller vocabulary than with CountVectorizer model because we used min_df=5\n",
    "\n",
    "tfidf_feature_names = np.array(tfidf_vect.get_feature_names())\n",
    "len(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build document-term matrices\n",
    "\n",
    "X_train_vectorized_2 = tfidf_vect.transform(X_train)\n",
    "X_test_vectorized_2 = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build the classification model (classifier)\n",
    "\n",
    "LR_model_2 = LogisticRegression()\n",
    "LR_model_2.fit(X_train_vectorized_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76870748299319724"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate the classifier, i.e. calculate its classification rate\n",
    "\n",
    "LR_predictions_2 = LR_model_2.predict(X_test_vectorized_2)\n",
    "accuracy_score(y_test, LR_predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model interpretation: which vocabulary words are most significant?\n",
    "\n",
    "#Sort vocabulary words according to their max tfidf feature value over all documents\n",
    "sorted_tfidf_index = X_train_vectorized_2.max(0).toarray()[0].argsort()\n",
    "\n",
    "#Sort vocabulary words according to their LR model coefficient value\n",
    "sorted_coef2_index = LR_model_2.coef_[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words sorted by tfidf feature value:\n",
      "  Smallest:\n",
      "['chay' 'm3ak' 'you' 'love' 'sans' 'هذا' 'femme' 'men' 'من' 'كان']\n",
      "\n",
      "  Largest: \n",
      "['يا' 'dima' 'masta' 'maquillage' 'mahlek' 'ma7lek' 'ma7leha' 'la' 'fi'\n",
      " 'est']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary words sorted by tfidf feature value:')\n",
    "print('  Smallest:\\n{}\\n'.format(tfidf_feature_names[sorted_tfidf_index[:10]]))\n",
    "print('  Largest: \\n{}'.format(tfidf_feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words sorted by LR model coefficient value:\n",
      "  Smallest Coefs:\n",
      "['3zouza' '5ayba' 'عزوزة' 'masta' 'maquillage' 'في' 't3awef' 'hhhhhh' 'ya'\n",
      " '3ifsha']\n",
      "\n",
      "  Largest Coefs: \n",
      "['belle' 'bayouta' 'ma7lek' 'mahlek' 'محلاك' '3asla' 'thabel' 'dima'\n",
      " 'ma7leha' '3lik']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary words sorted by LR model coefficient value:')\n",
    "print('  Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('  Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
