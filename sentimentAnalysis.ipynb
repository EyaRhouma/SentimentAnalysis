{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis using a collection of nearly half a million facebook reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import entire libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#library for plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "#so that plots appear in the same browser window\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Import specific items only from the sklearn library\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from a file\n",
    "df0 = pd.read_csv(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(780, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Message</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Sentiment_Category</th>\n",
       "      <th>url</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2076908612522082</td>\n",
       "      <td>Saida Hajji</td>\n",
       "      <td>bariiii</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1774334979305301</td>\n",
       "      <td>B≈ô√Ø≈à√Ø Mƒïƒè</td>\n",
       "      <td>Ya 3saaalllll a7la baya üòçüòòüòçüòò</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>4.4617E+14</td>\n",
       "      <td>Hossam Aouadi</td>\n",
       "      <td>ÿßŸÑÿ¨ŸÖÿßŸÑ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>3.13873E+14</td>\n",
       "      <td>Sofian Farhat</td>\n",
       "      <td>5ayba</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>7.50575E+14</td>\n",
       "      <td>Taki Eddine Meddeb</td>\n",
       "      <td>ŸÑÿß ŸÉŸÑÿßŸÖ ÿ®Ÿäÿ© üíì</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.facebook.com/Baya.Zardii/photos/a....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                Id                Name  \\\n",
       "0  2018-01-02  2076908612522082         Saida Hajji   \n",
       "1  2018-01-02  1774334979305301           B≈ô√Ø≈à√Ø Mƒïƒè   \n",
       "2  2018-01-02        4.4617E+14       Hossam Aouadi   \n",
       "3  2018-01-02       3.13873E+14       Sofian Farhat   \n",
       "4  2018-01-02       7.50575E+14  Taki Eddine Meddeb   \n",
       "\n",
       "                        Message  Like_Count  Comment_Count Sentiment_Category  \\\n",
       "0                       bariiii         0.0            0.0                 -1   \n",
       "1  Ya 3saaalllll a7la baya üòçüòòüòçüòò         0.0            0.0                  1   \n",
       "2                        ÿßŸÑÿ¨ŸÖÿßŸÑ         2.0            0.0                  1   \n",
       "3                         5ayba         1.0            0.0                 -1   \n",
       "4                 ŸÑÿß ŸÉŸÑÿßŸÖ ÿ®Ÿäÿ© üíì         1.0            0.0                  1   \n",
       "\n",
       "                                                 url  Unnamed: 8  Unnamed: 9  \n",
       "0  https://www.facebook.com/Baya.Zardii/photos/a....         NaN         NaN  \n",
       "1  https://www.facebook.com/Baya.Zardii/photos/a....         NaN         NaN  \n",
       "2  https://www.facebook.com/Baya.Zardii/photos/a....         NaN         NaN  \n",
       "3  https://www.facebook.com/Baya.Zardii/photos/a....         NaN         NaN  \n",
       "4  https://www.facebook.com/Baya.Zardii/photos/a....         NaN         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that this data consists of a matrix stored in a Pandas Data Frame\n",
    "#   containing one row per product and with columns = product attributes.\n",
    "print(type(df0))\n",
    "print(df0.shape)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 780 entries, 0 to 779\n",
      "Data columns (total 10 columns):\n",
      "Date                  776 non-null object\n",
      "Id                    776 non-null object\n",
      "Name                  776 non-null object\n",
      "Message               763 non-null object\n",
      "Like_Count            776 non-null float64\n",
      "Comment_Count         778 non-null float64\n",
      "Sentiment_Category    777 non-null object\n",
      "url                   780 non-null object\n",
      "Unnamed: 8            0 non-null float64\n",
      "Unnamed: 9            1 non-null float64\n",
      "dtypes: float64(4), object(6)\n",
      "memory usage: 61.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#We can also use the info() method to obtain a summary of a pandas Data Frame\n",
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-93d685272692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In order to speed up computation, we'll only use a small random sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#   of this data comprising 10 % of the rows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2137\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   4863\u001b[0m                              \"provide positive value.\")\n\u001b[1;32m   4864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4865\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4866\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0"
     ]
    }
   ],
   "source": [
    "# In order to speed up computation, we'll only use a small random sample\n",
    "#   of this data comprising 10 % of the rows.\n",
    "df = df0.sample(frac=0.1, random_state=2137)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 10)\n"
     ]
    }
   ],
   "source": [
    "# remove missing values by calling dropna() method\n",
    "#   arguments:  axis=0, how='any'  <=> remove all rows containing missing value in *any* column\n",
    "df0.dropna(inplace=True, axis=0, how='any')\n",
    "print(df0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "?df.dropna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and understand the data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of numeric attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.124663</td>\n",
       "      <td>0.400840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Like_Count  Comment_Count  Unnamed: 8  Unnamed: 9\n",
       "count   77.000000      78.000000         0.0         1.0\n",
       "mean     0.298701       0.089744         NaN         1.0\n",
       "std      1.124663       0.400840         NaN         NaN\n",
       "min      0.000000       0.000000         NaN         1.0\n",
       "25%      0.000000       0.000000         NaN         1.0\n",
       "50%      0.000000       0.000000         NaN         1.0\n",
       "75%      0.000000       0.000000         NaN         1.0\n",
       "max      9.000000       3.000000         NaN         1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The describe() method gives a numeric summary of numeric columns (attributes) only\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of brand names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(df0['Sentiment_Category']).most_common()\n",
    "\n",
    "#number of unique brand names\n",
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequent brand names in the dataset\n",
    "counter[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of product names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "counter2 = Counter(df['Like_Count']).most_common()\n",
    "\n",
    "print(len(counter2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 65), (1.0, 9), (9.0, 1), (nan, 1), (3.0, 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 Most frequent product names in the dataset\n",
    "counter2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of rating values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gR = df0.groupby('Sentiment_Category').size()\n",
    "print(type(gR))\n",
    "gR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 0 artists>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(gR.index, gR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new binary ratings variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 10)\n"
     ]
    }
   ],
   "source": [
    "# Remove any 'neutral' ratings equal to 3\n",
    "df0 = df0[df0['Sentiment_Category'] != 0]\n",
    "\n",
    "print(df0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Like_Count  Comment_Count  Unnamed: 8  Unnamed: 9\n",
       "count         0.0            0.0         0.0         0.0\n",
       "mean          NaN            NaN         NaN         NaN\n",
       "std           NaN            NaN         NaN         NaN\n",
       "min           NaN            NaN         NaN         NaN\n",
       "25%           NaN            NaN         NaN         NaN\n",
       "50%           NaN            NaN         NaN         NaN\n",
       "75%           NaN            NaN         NaN         NaN\n",
       "max           NaN            NaN         NaN         NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Message</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Sentiment_Category</th>\n",
       "      <th>url</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Id, Name, Message, Like_Count, Comment_Count, Sentiment_Category, url, Unnamed: 8, Unnamed: 9]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some examples with high ratings\n",
    "df[df['Sentiment_Category'] == -1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "- a. Print products that have a \"neutral\" rating, i.e. a rating of 1\n",
    "- b. Print products that have a \"positive\" rating, i.e. rating of 4 or 5\n",
    "- c. Print products that have a \"negative\" rating, i.e. rating of 1 or 2\n",
    "- d. Print number of products of the brand name **Apple**  have a \"positive\" rating and how many have a \"negative\" rating   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "?train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df0['Message'], \n",
    "                                                    df0['Sentiment_Category'], \n",
    "                                                    random_state=591)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Message, dtype: object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Sentiment_Category, dtype: object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis based on BOW model with word occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Feature extraction* means representing raw text documents as numerical *feature vectors*.\n",
    "- In the simple BOW model, feature vector = number of word occurrences for each document and each vocabulary word.\n",
    "- We will do this using the ``CountVectorizer`` class: first we'll **tokenize** the documents and extract the vocabulary set, and then we determine the feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize documents & build vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "#  i.e. learn the vocabulary (distinct words) of the input corpus\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '100',\n",
       " '100000',\n",
       " '16',\n",
       " '170',\n",
       " '1961',\n",
       " '20',\n",
       " '2ans',\n",
       " '2ara2',\n",
       " '30',\n",
       " '3adeya',\n",
       " '3ahra',\n",
       " '3ala',\n",
       " '3alekher',\n",
       " '3alik',\n",
       " '3all√©5r',\n",
       " '3am',\n",
       " '3amalit',\n",
       " '3amaliyet',\n",
       " '3amalyet',\n",
       " '3amla',\n",
       " '3amletlo',\n",
       " '3amlettou',\n",
       " '3amlthoum',\n",
       " '3and',\n",
       " '3andik',\n",
       " '3ar',\n",
       " '3asela',\n",
       " '3asla',\n",
       " '3assla',\n",
       " '3assoula',\n",
       " '3asssla',\n",
       " '3aych',\n",
       " '3az3ezti',\n",
       " '3bed',\n",
       " '3efcha',\n",
       " '3fata9',\n",
       " '3ifsha',\n",
       " '3ijbitni',\n",
       " '3in',\n",
       " '3inek',\n",
       " '3ini',\n",
       " '3inik',\n",
       " '3iyn',\n",
       " '3jbni',\n",
       " '3l5r',\n",
       " '3la',\n",
       " '3lch',\n",
       " '3leha',\n",
       " '3liha',\n",
       " '3liik',\n",
       " '3lik',\n",
       " '3lina',\n",
       " '3llikk',\n",
       " '3malt',\n",
       " '3morha',\n",
       " '3myen',\n",
       " '3omreha',\n",
       " '3omrek',\n",
       " '3omrha',\n",
       " '3omrk',\n",
       " '3sal',\n",
       " '3sayla',\n",
       " '3thertk',\n",
       " '3zoooooooouza',\n",
       " '3zouuuza',\n",
       " '3zouza',\n",
       " '3zza',\n",
       " '40',\n",
       " '4alina',\n",
       " '4och',\n",
       " '56',\n",
       " '57',\n",
       " '5aiba',\n",
       " '5ale9',\n",
       " '5arya',\n",
       " '5atrk',\n",
       " '5ayba',\n",
       " '5elti',\n",
       " '5fif',\n",
       " '5iiiiiiir',\n",
       " '5iiitt',\n",
       " '5ii√Æit',\n",
       " '5ir',\n",
       " '5irlk',\n",
       " '5it',\n",
       " '5kg',\n",
       " '5yout',\n",
       " '63',\n",
       " '65',\n",
       " '7a9',\n",
       " '7aja',\n",
       " '7ajja',\n",
       " '7ala',\n",
       " '7asdinek',\n",
       " '7atta',\n",
       " '7lili',\n",
       " '7loua',\n",
       " '7lowa',\n",
       " '7nekha',\n",
       " '7nina',\n",
       " '7ob',\n",
       " '7osed',\n",
       " '7othlet',\n",
       " '7wajbha',\n",
       " '7wajbk',\n",
       " '7wajeb',\n",
       " '8alta',\n",
       " '8atsa',\n",
       " '8irek',\n",
       " '9ad',\n",
       " '9adakch',\n",
       " '9adekch',\n",
       " '9albek',\n",
       " '9alet',\n",
       " '9alo',\n",
       " '9alou',\n",
       " '9asdeh',\n",
       " '9asdk',\n",
       " '9bal',\n",
       " '9dh',\n",
       " '9odm',\n",
       " '9ofa',\n",
       " '9ol',\n",
       " '9ololek',\n",
       " '9oul',\n",
       " 'a3leha',\n",
       " 'a3lem',\n",
       " 'a3mel',\n",
       " 'a3mell',\n",
       " 'a3rf',\n",
       " 'a5tini',\n",
       " 'a7lowa',\n",
       " 'aaaaa',\n",
       " 'aaaazouza',\n",
       " 'aba3tile',\n",
       " 'adore',\n",
       " 'ah',\n",
       " 'ahla',\n",
       " 'ak',\n",
       " 'akther',\n",
       " 'ala',\n",
       " 'alah',\n",
       " 'alger',\n",
       " 'alghalb',\n",
       " 'ali',\n",
       " 'aliha',\n",
       " 'alik',\n",
       " 'allah',\n",
       " 'always',\n",
       " 'ama',\n",
       " 'aman',\n",
       " 'amel',\n",
       " 'amina',\n",
       " 'amoura',\n",
       " 'amrouche',\n",
       " 'amset',\n",
       " 'amtelk',\n",
       " 'ana',\n",
       " 'ans',\n",
       " 'aslha',\n",
       " 'avec',\n",
       " 'awil',\n",
       " 'aya',\n",
       " 'ayab',\n",
       " 'azeyane',\n",
       " 'azouza',\n",
       " 'azyan',\n",
       " 'azyeen',\n",
       " 'azyen',\n",
       " 'azzyin',\n",
       " 'b1000000000',\n",
       " 'ba3thek',\n",
       " 'bagra',\n",
       " 'bahbouha',\n",
       " 'bahom',\n",
       " 'bara',\n",
       " 'barcha',\n",
       " 'barii',\n",
       " 'bariiii',\n",
       " 'barka',\n",
       " 'bassem',\n",
       " 'bay',\n",
       " 'baya',\n",
       " 'bayna',\n",
       " 'bayota',\n",
       " 'bayouna',\n",
       " 'bayouta',\n",
       " 'bayoutaa',\n",
       " 'bayoutaaaa',\n",
       " 'bayouutta',\n",
       " 'bayya',\n",
       " 'bayyouta',\n",
       " 'bb',\n",
       " 'beauty',\n",
       " 'beaut√©',\n",
       " 'bech',\n",
       " 'beel',\n",
       " 'behia',\n",
       " 'bel',\n",
       " 'bel7a9',\n",
       " 'belal',\n",
       " 'bell',\n",
       " 'bella',\n",
       " 'belle',\n",
       " 'bellessima',\n",
       " 'bello',\n",
       " 'ben',\n",
       " 'benesba',\n",
       " 'benti',\n",
       " 'berah',\n",
       " 'berge',\n",
       " 'beya',\n",
       " 'beyen',\n",
       " 'beyouta',\n",
       " 'bhar',\n",
       " 'bhima',\n",
       " 'bien',\n",
       " 'bih',\n",
       " 'bihom',\n",
       " 'bil',\n",
       " 'bilhak',\n",
       " 'bkolha',\n",
       " 'bla',\n",
       " 'blach',\n",
       " 'blech',\n",
       " 'blmahleek',\n",
       " 'bnaya',\n",
       " 'bnt',\n",
       " 'bnty',\n",
       " 'boba',\n",
       " 'bon',\n",
       " 'botox',\n",
       " 'bou',\n",
       " 'boumerdes',\n",
       " 'brasmi',\n",
       " 'bravo',\n",
       " 'bravoooooooooooooooo',\n",
       " 'briyon',\n",
       " 'brou7ek',\n",
       " 'bsara7a',\n",
       " 'bsaraha',\n",
       " 'byottttta',\n",
       " 'byouta',\n",
       " 'cava',\n",
       " 'ch3irkk',\n",
       " 'cha',\n",
       " 'cha3erk',\n",
       " 'cha5sitha',\n",
       " 'chab3a',\n",
       " 'chabeb',\n",
       " 'chadit',\n",
       " 'chai',\n",
       " 'chanti',\n",
       " 'charmantes',\n",
       " 'charmonte',\n",
       " 'chay',\n",
       " 'che7ra9',\n",
       " 'cheda',\n",
       " 'chedde',\n",
       " 'chibb',\n",
       " 'chkoun',\n",
       " 'chleka',\n",
       " 'chneya',\n",
       " 'chnwa',\n",
       " 'chofetha',\n",
       " 'choftek',\n",
       " 'chwaya',\n",
       " 'chyb',\n",
       " 'claaaaaaas',\n",
       " 'clair',\n",
       " 'classe',\n",
       " 'col√®ration',\n",
       " 'coureg',\n",
       " 'cute',\n",
       " 'da3wa',\n",
       " 'dana',\n",
       " 'dari',\n",
       " 'dawem',\n",
       " 'dbachha',\n",
       " 'de',\n",
       " 'deco',\n",
       " 'deleni',\n",
       " 'denia',\n",
       " 'dhaher',\n",
       " 'dhahra',\n",
       " 'dhou9',\n",
       " 'dhou9ik',\n",
       " 'dima',\n",
       " 'din',\n",
       " 'doute',\n",
       " 'dr1',\n",
       " 'dz',\n",
       " 'd√©nia',\n",
       " 'e7tirami',\n",
       " 'ebyoutna',\n",
       " 'el',\n",
       " 'elegante',\n",
       " 'eli',\n",
       " 'elkol',\n",
       " 'elle',\n",
       " 'elli',\n",
       " 'elmezyen',\n",
       " 'elte3ml',\n",
       " 'ely',\n",
       " 'ema',\n",
       " 'emmchii',\n",
       " 'emmm',\n",
       " 'en',\n",
       " 'ena',\n",
       " 'encore',\n",
       " 'enhbha',\n",
       " 'enjasttouna',\n",
       " 'enrob√©',\n",
       " 'enti',\n",
       " 'entouma',\n",
       " 'enty',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'etfate9',\n",
       " 'ey',\n",
       " 'eyy',\n",
       " 'f3lk',\n",
       " 'faha',\n",
       " 'fak',\n",
       " 'fama',\n",
       " 'far5aa',\n",
       " 'far7an',\n",
       " 'fawzi',\n",
       " 'fe',\n",
       " 'feha',\n",
       " 'fehri',\n",
       " 'femme',\n",
       " 'femmme',\n",
       " 'fenia',\n",
       " 'fhemtk',\n",
       " 'fi',\n",
       " 'fih',\n",
       " 'fiha',\n",
       " 'fihh',\n",
       " 'fik',\n",
       " 'fil',\n",
       " 'fin',\n",
       " 'fina',\n",
       " 'fi√®rent',\n",
       " 'fom',\n",
       " 'fond',\n",
       " 'fort',\n",
       " 'foumk',\n",
       " 'good',\n",
       " 'grave',\n",
       " 'ha',\n",
       " 'haadhika',\n",
       " 'haguar',\n",
       " 'hahahahaha',\n",
       " 'hailayt',\n",
       " 'haja',\n",
       " 'hajja',\n",
       " 'hak',\n",
       " 'haka',\n",
       " 'hal',\n",
       " 'hala',\n",
       " 'harri99',\n",
       " 'hata',\n",
       " 'hathi',\n",
       " 'hawinek',\n",
       " 'haylaaa',\n",
       " 'hbel',\n",
       " 'hedhoukem',\n",
       " 'hedi',\n",
       " 'heja',\n",
       " 'hek',\n",
       " 'heka',\n",
       " 'heki',\n",
       " 'hetha',\n",
       " 'hethi',\n",
       " 'hh',\n",
       " 'hhh',\n",
       " 'hhhh',\n",
       " 'hhhhh',\n",
       " 'hhhhhh',\n",
       " 'hhhhhhh',\n",
       " 'hhhhhhhhh',\n",
       " 'hhhhhhhhhh',\n",
       " 'hhhhhhhhhhh',\n",
       " 'hhhhhhhhhhhhhhhhh',\n",
       " 'hia',\n",
       " 'hindem',\n",
       " 'hiya',\n",
       " 'hlewa',\n",
       " 'hlouwa',\n",
       " 'hob',\n",
       " 'houni',\n",
       " 'houwa',\n",
       " 'howa',\n",
       " 'h√©ki',\n",
       " 'i7chem',\n",
       " 'i7chm',\n",
       " 'idraz',\n",
       " 'ih',\n",
       " 'il',\n",
       " 'ili',\n",
       " 'ill',\n",
       " 'ilmou5if',\n",
       " 'inte',\n",
       " 'inti',\n",
       " 'intiii',\n",
       " 'in√©gal√©',\n",
       " 'is7i7',\n",
       " 'itwada3',\n",
       " 'itwansa',\n",
       " 'jamehir',\n",
       " 'jarna',\n",
       " 'jidiya',\n",
       " 'jidya',\n",
       " 'jloudha',\n",
       " 'jmai',\n",
       " 'jolie',\n",
       " 'jolies',\n",
       " 'jwayed',\n",
       " 'ka7a',\n",
       " 'kalb',\n",
       " 'kan',\n",
       " 'karde≈üim',\n",
       " 'karim',\n",
       " 'kathaba',\n",
       " 'kbert',\n",
       " 'kbira',\n",
       " 'kdeba',\n",
       " 'kebret',\n",
       " 'kelmtyn',\n",
       " 'ken',\n",
       " 'khali',\n",
       " 'khaterha',\n",
       " 'khawla',\n",
       " 'khawte',\n",
       " 'khayba',\n",
       " 'ki',\n",
       " 'kif',\n",
       " 'kifech',\n",
       " 'kil',\n",
       " 'kima',\n",
       " 'klba',\n",
       " 'kn',\n",
       " 'kol',\n",
       " 'koul',\n",
       " 'kƒ±z',\n",
       " 'l3ada',\n",
       " 'l3iiib',\n",
       " 'l3yb',\n",
       " 'l7ayet',\n",
       " 'l9ird',\n",
       " 'la',\n",
       " 'la3bed',\n",
       " 'lachkel',\n",
       " 'laha',\n",
       " 'layla',\n",
       " 'le3b',\n",
       " 'lebhim',\n",
       " 'lebsa',\n",
       " 'lel',\n",
       " 'lezm',\n",
       " 'li',\n",
       " 'lik',\n",
       " 'likoum',\n",
       " 'ljazair',\n",
       " 'lkithb',\n",
       " 'lkol',\n",
       " 'lmitkabir',\n",
       " 'lmot7f',\n",
       " 'lmra',\n",
       " 'lokhrin',\n",
       " 'look',\n",
       " 'lotf',\n",
       " 'love',\n",
       " 'lrabi',\n",
       " 'm3a',\n",
       " 'm3aha',\n",
       " 'm3ak',\n",
       " 'm3arsa',\n",
       " 'm3nha',\n",
       " 'm5ybk',\n",
       " 'm7alk',\n",
       " 'm7lllllak',\n",
       " 'ma',\n",
       " 'ma2arwa3ak',\n",
       " 'ma3adech',\n",
       " 'ma3andhomech',\n",
       " 'ma3jbo',\n",
       " 'ma3raftekch',\n",
       " 'ma5ibik',\n",
       " 'ma5lou9a',\n",
       " 'ma5ybik',\n",
       " 'ma5yeb',\n",
       " 'ma7boubat',\n",
       " 'ma7la',\n",
       " 'ma7laha',\n",
       " 'ma7lak',\n",
       " 'ma7leha',\n",
       " 'ma7lehak',\n",
       " 'ma7lek',\n",
       " 'ma7lekk',\n",
       " 'ma7lik',\n",
       " 'ma7lk',\n",
       " 'ma8iir',\n",
       " 'mabheha',\n",
       " 'machalah',\n",
       " 'machalh',\n",
       " 'machalla',\n",
       " 'machallah',\n",
       " 'macyaj',\n",
       " 'mafama',\n",
       " 'mafameche',\n",
       " 'maguir',\n",
       " 'mahlaik',\n",
       " 'mahlak',\n",
       " 'mahleek',\n",
       " 'mahleha',\n",
       " 'mahlek',\n",
       " 'mahlekk',\n",
       " 'mahllek',\n",
       " 'mahyech',\n",
       " 'mais',\n",
       " 'mak',\n",
       " 'make',\n",
       " 'makiage',\n",
       " 'makija',\n",
       " 'makillage',\n",
       " 'makup',\n",
       " 'makyage',\n",
       " 'makyaje',\n",
       " 'makyajek',\n",
       " 'mal9it',\n",
       " 'mal9itt',\n",
       " 'mala',\n",
       " 'maman',\n",
       " 'mameti',\n",
       " 'mamety',\n",
       " 'man77ebikish',\n",
       " 'man7ebech',\n",
       " 'man9oul',\n",
       " 'mandher',\n",
       " 'mandhir',\n",
       " 'mani',\n",
       " 'manther',\n",
       " 'maq',\n",
       " 'maqeillege',\n",
       " 'maqiage',\n",
       " 'maquiallage',\n",
       " 'maquiallge',\n",
       " 'maquillage',\n",
       " 'maquillages',\n",
       " 'mara',\n",
       " 'marabetek',\n",
       " 'mariem',\n",
       " 'marra',\n",
       " 'masali',\n",
       " 'mascara',\n",
       " 'masta',\n",
       " 'mastaaaaaaaaaaaaa',\n",
       " 'mastoura',\n",
       " 'mat7a9etlich',\n",
       " 'mata7chemech',\n",
       " 'matafla7',\n",
       " 'matasla7',\n",
       " 'mati3jibnich',\n",
       " 'mauve',\n",
       " 'maydoumlk',\n",
       " 'maye3nouli',\n",
       " 'mazelt',\n",
       " 'mazinek',\n",
       " 'mchit',\n",
       " 'mdr',\n",
       " 'mdrrrrrr',\n",
       " 'me',\n",
       " 'me3arestch',\n",
       " 'mech',\n",
       " 'meghiara',\n",
       " 'mel',\n",
       " 'mela',\n",
       " 'melehbel',\n",
       " 'memi',\n",
       " 'men',\n",
       " 'men8ir',\n",
       " 'menha',\n",
       " 'meni',\n",
       " 'menn',\n",
       " 'merci',\n",
       " 'meteswe',\n",
       " 'metetmena',\n",
       " 'meziaaaaaana',\n",
       " 'meziana',\n",
       " 'meziena',\n",
       " 'mezyena',\n",
       " 'mezzzzziena',\n",
       " 'mgalgetni',\n",
       " 'mhlek',\n",
       " 'mi',\n",
       " 'miboun',\n",
       " 'michay',\n",
       " 'migale',\n",
       " 'min',\n",
       " 'mine',\n",
       " 'minek',\n",
       " 'minha',\n",
       " 'minik',\n",
       " 'mit',\n",
       " 'mitkabra',\n",
       " 'mizyana',\n",
       " 'mjalda',\n",
       " 'mkch',\n",
       " 'mlakez',\n",
       " 'mmmm',\n",
       " 'mmmma7lek',\n",
       " 'mn',\n",
       " 'mnawra',\n",
       " 'mnawrin',\n",
       " 'mo5i',\n",
       " 'moch',\n",
       " 'mohamed',\n",
       " 'mohsenn',\n",
       " 'mojttama3',\n",
       " 'morsi',\n",
       " 'mouch',\n",
       " 'mouche',\n",
       " 'mouchek',\n",
       " 'mounachta',\n",
       " 'mouthi3',\n",
       " 'mr6',\n",
       " 'mrama',\n",
       " 'mrc',\n",
       " 'msam7a',\n",
       " 'msikna',\n",
       " 'mta3',\n",
       " 'mthabta',\n",
       " 'mtswa',\n",
       " 'mtt7rkch',\n",
       " 'mwahra',\n",
       " 'mwhra',\n",
       " 'mzyena',\n",
       " 'mzyna',\n",
       " 'm√™la',\n",
       " 'm√™me',\n",
       " 'n3ref',\n",
       " 'n7bk',\n",
       " 'n7bok',\n",
       " 'n9olha',\n",
       " 'na7i',\n",
       " 'na7it',\n",
       " 'na7na',\n",
       " 'nabi',\n",
       " 'nahdhr',\n",
       " 'nahkouch',\n",
       " 'nakirhiik',\n",
       " 'nakirhik',\n",
       " 'nakraha',\n",
       " 'nakrehek',\n",
       " 'nbkhro',\n",
       " 'nchala',\n",
       " 'nchouf',\n",
       " 'nchoufouk',\n",
       " 'ne',\n",
       " 'nertehou',\n",
       " 'nes',\n",
       " 'nesre9',\n",
       " 'netttttttna',\n",
       " 'nhabkom',\n",
       " 'nhar',\n",
       " 'nhb',\n",
       " 'nhebek',\n",
       " 'nhebha',\n",
       " 'nhebk',\n",
       " 'nheebha',\n",
       " 'nhibek',\n",
       " 'nice',\n",
       " 'nil',\n",
       " 'nji',\n",
       " 'nkrha',\n",
       " 'nmout',\n",
       " 'nn',\n",
       " 'non',\n",
       " 'normale',\n",
       " 'noufel',\n",
       " 'nte9',\n",
       " 'nude',\n",
       " 'ok',\n",
       " 'okhti',\n",
       " 'omezyena',\n",
       " 'omi',\n",
       " 'on',\n",
       " 'osra',\n",
       " 'ou',\n",
       " 'oui',\n",
       " 'oumor',\n",
       " 'oumour',\n",
       " 'ouuhh',\n",
       " 'panso',\n",
       " 'pas',\n",
       " 'pauvre',\n",
       " 'pinceau',\n",
       " 'plus',\n",
       " 'quand',\n",
       " 'ra9tek',\n",
       " 'rabbi',\n",
       " 'rabetek',\n",
       " 'rabi',\n",
       " 'raby',\n",
       " 'rahi',\n",
       " 'rak',\n",
       " 'rak3a',\n",
       " 'ram',\n",
       " 'rani',\n",
       " 'rasmethoum',\n",
       " 'ravissante',\n",
       " 'raw',\n",
       " 'rawdha',\n",
       " 'raww',\n",
       " 'regarde',\n",
       " 'retraite',\n",
       " 'richa',\n",
       " 'rjou3',\n",
       " 'rou7aa',\n",
       " 'rou7k',\n",
       " 'rou7√©k',\n",
       " 'rouge',\n",
       " 'rouhek',\n",
       " 'sadek',\n",
       " 'sadka',\n",
       " 'sajda',\n",
       " 'sajeda',\n",
       " 'salam',\n",
       " 'salami',\n",
       " 'salit',\n",
       " 'sami',\n",
       " 'sans',\n",
       " 'saya',\n",
       " 'saybb',\n",
       " 'sayeb',\n",
       " 'sa√Ød',\n",
       " 'se',\n",
       " 'seb7a',\n",
       " 'sebha',\n",
       " 'seni',\n",
       " 'sensible',\n",
       " 'serwelk',\n",
       " 'seviyorum',\n",
       " 'sfiiha',\n",
       " 'shay',\n",
       " 'shih',\n",
       " 'shnwa',\n",
       " 'si',\n",
       " 'simple',\n",
       " 'simplicit√©',\n",
       " 'sne',\n",
       " 'snin',\n",
       " 'sob7an',\n",
       " 'sob7nou',\n",
       " 'sobhan',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'soussa',\n",
       " 'spontan√©',\n",
       " 'spontan√©e',\n",
       " 'srrt',\n",
       " 'suuper',\n",
       " 's√©mm7a',\n",
       " 't3awef',\n",
       " 't3jebni',\n",
       " 't7eb',\n",
       " 't7ibo',\n",
       " 't9olhom',\n",
       " 't9oul',\n",
       " 'ta3tou',\n",
       " 'ta4ou',\n",
       " 'ta7founa',\n",
       " 'tab9a',\n",
       " 'tabdi',\n",
       " 'tabrkla',\n",
       " 'tahiyate',\n",
       " 'tais',\n",
       " 'tajmil',\n",
       " 'tamou7a',\n",
       " 'tanja7',\n",
       " 'taraji',\n",
       " 'taswira',\n",
       " 'tata',\n",
       " 'taw',\n",
       " 'tawa',\n",
       " 'tawaaa',\n",
       " 'tawba',\n",
       " 'tay7a',\n",
       " 'tayara',\n",
       " 'tb9',\n",
       " 'tb9i',\n",
       " 'tbarkala',\n",
       " 'tbarkalah',\n",
       " 'tbarkalla',\n",
       " 'tbarkallah',\n",
       " 'tchabah',\n",
       " 'tchbah',\n",
       " 'tched',\n",
       " 'te3jebni',\n",
       " 'te7bek',\n",
       " 'te7chem',\n",
       " 'techrab',\n",
       " 'tekdeb',\n",
       " 'tekdheb',\n",
       " 'tesghar',\n",
       " 'tetghchech',\n",
       " 'tetsana3',\n",
       " 'tfata9',\n",
       " 'tfete9',\n",
       " 'tfj3iiiiiiiiiiiii',\n",
       " 'tgoli',\n",
       " 'tha9afa',\n",
       " 'thaballl',\n",
       " 'thabbil',\n",
       " 'thabel',\n",
       " 'thabele',\n",
       " 'thabil',\n",
       " 'thechi',\n",
       " 'thnitek',\n",
       " 'ti',\n",
       " 'tii',\n",
       " 'tijri',\n",
       " 'tir',\n",
       " 'tirr',\n",
       " 'tirran',\n",
       " 'tji',\n",
       " 'tjmil',\n",
       " 'tjr',\n",
       " 'tkark',\n",
       " 'tkoli',\n",
       " 'tmanitha',\n",
       " 'to9tel',\n",
       " 'to9tell',\n",
       " 'tod5ol',\n",
       " 'todhhor',\n",
       " 'toi',\n",
       " 'tok3ed',\n",
       " 'toooooooooop',\n",
       " 'tooooop',\n",
       " 'toooppp',\n",
       " 'top',\n",
       " 'toppp',\n",
       " 'torbya',\n",
       " 'tou9tel',\n",
       " 'toub',\n",
       " 'toujours',\n",
       " 'toumou7',\n",
       " 'tounes',\n",
       " 'touns',\n",
       " 'tres',\n",
       " 'trop',\n",
       " 'tr√®s',\n",
       " 'tskhayel',\n",
       " 'tt9o7b',\n",
       " 'ttsanna3',\n",
       " 'tu',\n",
       " 'tw',\n",
       " 'twa',\n",
       " 'ty',\n",
       " 'tz3bin',\n",
       " 'tzakem',\n",
       " 'tzid',\n",
       " 'ui',\n",
       " 'uii',\n",
       " 'une',\n",
       " 'up',\n",
       " 'vert',\n",
       " 'very',\n",
       " 'voil√†',\n",
       " 'vousetestresbelle',\n",
       " 'vrai',\n",
       " 'vraiment',\n",
       " 'w3andik',\n",
       " 'w5doudek',\n",
       " 'w9ayet',\n",
       " 'w9ayett',\n",
       " 'wa',\n",
       " 'wa7ch',\n",
       " 'wahad',\n",
       " 'wahra',\n",
       " 'wal7i',\n",
       " 'wala',\n",
       " 'walah',\n",
       " 'walahi',\n",
       " 'walet',\n",
       " 'walh',\n",
       " 'wallah',\n",
       " 'wallh',\n",
       " 'wassel',\n",
       " 'waw',\n",
       " 'wbbarra',\n",
       " 'we',\n",
       " 'we7la',\n",
       " 'we9tech',\n",
       " 'wejha',\n",
       " 'wejhek',\n",
       " 'wejhk',\n",
       " 'wela',\n",
       " 'weli',\n",
       " 'whla3ba',\n",
       " 'wi9tha',\n",
       " 'wili',\n",
       " 'willayat',\n",
       " 'win',\n",
       " 'wklo8a',\n",
       " 'wlh',\n",
       " 'wma7la',\n",
       " 'wmohtarma',\n",
       " 'wodhnii',\n",
       " 'womniti',\n",
       " 'woooh',\n",
       " 'woooo',\n",
       " 'wor9od',\n",
       " 'wrabi',\n",
       " 'wrzina',\n",
       " 'wtsyb',\n",
       " 'y3awed',\n",
       " 'y3ayet',\n",
       " 'y5alini',\n",
       " 'y9olk',\n",
       " 'ya',\n",
       " 'ya3tek',\n",
       " 'ya3tik',\n",
       " 'ya7mik',\n",
       " 'yahafdhak',\n",
       " 'yahhafdhak',\n",
       " 'yakhi',\n",
       " 'yaser',\n",
       " 'ye',\n",
       " 'yena3en',\n",
       " 'yeser',\n",
       " 'yetsawrouh',\n",
       " 'yezi',\n",
       " 'yheloulha',\n",
       " 'yjiboulha',\n",
       " 'ykarker',\n",
       " 'ykeberne',\n",
       " 'ym3lm',\n",
       " 'you',\n",
       " 'youki',\n",
       " 'ytfach',\n",
       " 'ytsal',\n",
       " 'ywaf9ak',\n",
       " 'ywafkik',\n",
       " 'yzi',\n",
       " 'za3ma',\n",
       " 'zaid',\n",
       " 'zardi',\n",
       " 'zayn',\n",
       " 'zeda',\n",
       " 'zid',\n",
       " 'zin',\n",
       " 'zinek',\n",
       " 'zinha',\n",
       " 'zinik',\n",
       " 'zink',\n",
       " 'zinkkk',\n",
       " 'zitou',\n",
       " 'ziyn',\n",
       " '√©7taram',\n",
       " '√©mission',\n",
       " 'ÿ¢ÿ¥',\n",
       " 'ÿ£ÿ¥ÿ®ÿßŸá',\n",
       " 'ÿ£ÿ¥ÿ±ŸÅ',\n",
       " 'ÿ£ŸÖŸä',\n",
       " 'ÿ£ŸÜÿß',\n",
       " 'ÿ£ŸÜŸäŸÇÿ©',\n",
       " 'ÿ•ÿ®ÿπÿØŸä',\n",
       " 'ÿ•ŸÜÿ™Ÿä',\n",
       " 'ÿßÿ°ÿ®ÿ™ÿ≥ÿß',\n",
       " 'ÿßÿ°Ÿäÿ™Ÿáÿß',\n",
       " 'ÿßÿ®ŸÜ',\n",
       " 'ÿßÿ™ŸÇŸàÿß',\n",
       " 'ÿßÿ≠ÿ≥ŸÜ',\n",
       " 'ÿßÿ≠ŸÑÿß',\n",
       " 'ÿßÿ≠ŸÑŸä',\n",
       " 'ÿßÿÆÿ™Ÿä',\n",
       " 'ÿßÿ±Ÿàÿ≠',\n",
       " 'ÿßÿπŸÑŸäŸáÿß',\n",
       " 'ÿßŸÇŸÑ',\n",
       " 'ÿßŸÉÿ®ÿ±',\n",
       " 'ÿßŸÉŸäŸäŸäŸäÿØ',\n",
       " 'ÿßŸÑÿßÿ≠ÿØ',\n",
       " 'ÿßŸÑÿßÿÆŸÑÿßŸÇ',\n",
       " 'ÿßŸÑÿßÿÆŸäÿ±',\n",
       " 'ÿßŸÑÿßŸàŸÇÿßÿ™',\n",
       " 'ÿßŸÑÿ®ŸÑÿßÿØÿ©',\n",
       " 'ÿßŸÑÿ™ÿ±ÿ®Ÿäÿ©',\n",
       " 'ÿßŸÑÿ™ŸÅÿßŸáÿ©',\n",
       " 'ÿßŸÑÿ™ŸÖÿßÿ≥Ÿäÿ≠',\n",
       " 'ÿßŸÑÿ™ŸàŸÜÿ≥Ÿä',\n",
       " 'ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ±',\n",
       " 'ÿßŸÑÿ¨ŸÖÿßŸÑ',\n",
       " 'ÿßŸÑÿ≠ÿµÿßŸÜ',\n",
       " 'ÿßŸÑÿÆÿßŸäÿ®ÿ©',\n",
       " 'ÿßŸÑÿÆÿ∂ÿ±ÿßÿ°',\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print words are there in the vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1445"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct document-term matrix\n",
    "- This matrix contains the *feature vectors* of a given set of raw documents.\n",
    "- For the simple BOW model, feature vector = number of word occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(438, 1445)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(147, 1445)\n"
     ]
    }
   ],
   "source": [
    "# the document-term matrix for the training corpus\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(type(X_train_vectorized))\n",
    "print(X_train_vectorized.shape)\n",
    "\n",
    "# the document-term matrix for the test corpus\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "print(type(X_test_vectorized))\n",
    "print(X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2119\n"
     ]
    }
   ],
   "source": [
    "# Number of non-zero elements in document-term matrix of training corpus\n",
    "print(X_train_vectorized.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the proportion of non-zero elements in ``X_train_vectorized`` document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033480273656602043"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.nnz / (X_train_vectorized.shape[0] * X_train_vectorized.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1445)\n",
      "1\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Number of training documents that contain each word (called document frequency) fema kelma wawjouda fi document bark w fema kelma mawjouda fi 24 doc\n",
    "doc_freq = np.array((X_train_vectorized > 0).sum(0))\n",
    "print(doc_freq.shape)\n",
    "print(np.amin(doc_freq))\n",
    "print(np.amax(doc_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1445)\n",
      "0.00228310502283\n",
      "0.0547945205479\n"
     ]
    }
   ],
   "source": [
    "# Proportion of training documents that contain each word (called relative document frequency)\n",
    "n,m = X_train_vectorized.shape\n",
    "rel_doc_freq = np.array((X_train_vectorized > 0).sum(0)/n)\n",
    "\n",
    "print(rel_doc_freq.shape)\n",
    "print(np.amin(rel_doc_freq))\n",
    "print(np.amax(rel_doc_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 1)\n",
      "0\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# Number of unique words in each training document/ fema doc fih 0 kemla w fema doc fih 38 kemla\n",
    "words_per_doc = np.array((X_train_vectorized > 0).sum(1))\n",
    "print(words_per_doc.shape)\n",
    "print(np.amin(words_per_doc))\n",
    "print(np.amax(words_per_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_per_doc[words_per_doc ==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.12290000e+04,   1.39000000e+03,   2.69000000e+02,\n",
       "          8.60000000e+01,   3.50000000e+01,   1.50000000e+01,\n",
       "          8.00000000e+00,   1.00000000e+00,   6.00000000e+00,\n",
       "          1.00000000e+00]),\n",
       " array([   0. ,   74.6,  149.2,  223.8,  298.4,  373. ,  447.6,  522.2,\n",
       "         596.8,  671.4,  746. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feedee4f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of number of unique words in each document\n",
    "plt.hist(words_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of number of document frequency of words\n",
    "# plt.hist(doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove documents with 0 words from training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411          ‚ù§üíñ\n",
       "555    J.t.m ‚ù§‚ù§\n",
       "356         üòåüòñüòñ\n",
       "642          ‚ù§‚ù§\n",
       "531          <3\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select documents that contain 0 words\n",
    "idx = np.where(words_per_doc == 0)[0]\n",
    "# Show all those documents\n",
    "X_train.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433,)\n"
     ]
    }
   ],
   "source": [
    "## Remove rows from training data that contain no words\n",
    "X_train = X_train.drop(X_train.index[idx])\n",
    "y_train = y_train.drop(y_train.index[idx])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 1445)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-compute the document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433, 1)\n",
      "1\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# verify results: re-compute number of words in each document\n",
    "words_per_doc = np.array((X_train_vectorized > 0).sum(1))\n",
    "print(words_per_doc.shape)\n",
    "print(np.amin(words_per_doc))\n",
    "print(np.amax(words_per_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classification model using Logistic Regression\n",
    "We are going to  to build a classification model using the feature vectors of the training documents (which are stored in the variable ``X_train_vectorized``) and their corresponding true sentiment categories (which are stored in the variable ``y_train``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using LR method\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the classification model\n",
    "We'll use the obtained LR model to predict sentiment categories (classes) of test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(147,)\n"
     ]
    }
   ],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "LR_predictions = LR_model.predict(X_test_vectorized)\n",
    "print(type(LR_predictions))\n",
    "print(LR_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.]\n",
      "[-1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(LR_predictions[:30])\n",
    "print(np.array(y_test[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the first 30 test documents, how many predictions are wrong?  *Hint*: see the output of the above cell.\n",
    "- For the first 1000 test documents, how many predictions are wrong?  *Hint*: use an expression of the form: ``np.sum(a == b)`` where a and b are two arrays of the same size.\n",
    "- What is the *classification rate* for the entire test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(147, 2)\n",
      "[[ 0.46485078  0.53514922]\n",
      " [ 0.16431827  0.83568173]\n",
      " [ 0.18042432  0.81957568]\n",
      " [ 0.27803192  0.72196808]\n",
      " [ 0.82038889  0.17961111]\n",
      " [ 0.71969634  0.28030366]\n",
      " [ 0.07019447  0.92980553]\n",
      " [ 0.30157635  0.69842365]\n",
      " [ 0.21246429  0.78753571]\n",
      " [ 0.32725931  0.67274069]]\n"
     ]
    }
   ],
   "source": [
    "# Actually, we can also obtain the prediction probabilities for each sentiment category\n",
    "LR_pred_prob = LR_model.predict_proba(X_test_vectorized)\n",
    "print(type(LR_pred_prob))\n",
    "print(LR_pred_prob.shape)\n",
    "\n",
    "# the prediction probabilities for the first 10 test documents\n",
    "print(LR_pred_prob[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance of classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78231292517006801"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate model's classification rate on the test corpus\n",
    "\n",
    "LR_classif_rate = accuracy_score(y_test, LR_predictions)\n",
    "LR_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Interpretation of model's coefficients (parameters)\n",
    "Print which vocabulary words are most important in our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1445,)\n"
     ]
    }
   ],
   "source": [
    "# first get LR model's coefficient (there is one coefficient per vocabulary word)\n",
    "coefs = LR_model.coef_[0]\n",
    "print(coefs.shape)\n",
    "\n",
    "# Sort these coefficient values in ascending order\n",
    "sorted_coef_index = coefs.argsort()  #sort by actual value\n",
    "sorted_coef_index_2 = abs(coefs).argsort()  #sort by absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs of LR model:\n",
      "\n",
      " Coefficient values:\n",
      "[-1.66356296 -1.62225352 -1.61714436 -1.28702415 -1.10875703 -0.99170879\n",
      " -0.90704645 -0.79517843 -0.77632431 -0.75710403]\n",
      "\n",
      " Feature names:\n",
      "['3zouza' '5ayba' 'ÿπÿ≤Ÿàÿ≤ÿ©' 'masta' 'maquillage' 'ŸÅŸä' 't3awef' 'hhhhhh' 'ya'\n",
      " '3ifsha']\n",
      "\n",
      "Largest Coefs of LR model:\n",
      "\n",
      " Coefficient values: \n",
      "[ 1.53688888  1.34706152  1.07814788  0.98554512  0.92528683  0.90997876\n",
      "  0.903053    0.83388748  0.78340395  0.76051351]\n",
      " Feature names: \n",
      "['belle' 'bayouta' 'ma7lek' 'mahlek' 'ŸÖÿ≠ŸÑÿßŸÉ' '3asla' 'thabel' 'dima'\n",
      " 'ma7leha' '3lik']\n",
      "Smallest abs(Coefs):\n",
      "\n",
      " Coefficient values:\n",
      "[ 0.00128754  0.0021675  -0.00221346  0.00270874  0.00270874  0.00270874\n",
      "  0.00270874  0.00270874  0.00270874  0.00270874]\n",
      "\n",
      " Feature names:\n",
      "['ÿßŸÑÿπŸÖÿ±' 'benti' 'hhhhhhhhhhhhhhhhh' 'mohamed' 'ljazair' 'salami'\n",
      " 'boumerdes' 'jidya' 'yahafdhak' 'bay']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the 10 smallest and 10 largest coefficients\n",
    "\n",
    "#feature_names = vect.get_feature_names()\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "print('Smallest Coefs of LR model:\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "\n",
    "print('Largest Coefs of LR model:\\n')\n",
    "print(' Coefficient values: \\n{}'.format(coefs[sorted_coef_index[:-11:-1]]))\n",
    "print(' Feature names: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "\n",
    "print('Smallest abs(Coefs):\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index_2[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index_2[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classification model using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Naive Bayes classification model\n",
    "\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this model to predict sentiment of test documents\n",
    "\n",
    "NB_predictions = NB_model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78911564625850339"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate model's classification rate on the test corpus\n",
    "\n",
    "NB_classif_rate = accuracy_score(y_test, NB_predictions)\n",
    "NB_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using BOW model and Tfidf\n",
    "We are basically going to re-do the same steps as above, but using the ``TfidfVectorizer`` class instead of ``CountVectorizer`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build vocabulary\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "tfidf_vect = TfidfVectorizer(min_df=5)\n",
    "tfidf_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we obtained a smaller vocabulary than with CountVectorizer model because we used min_df=5\n",
    "\n",
    "tfidf_feature_names = np.array(tfidf_vect.get_feature_names())\n",
    "len(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build document-term matrices\n",
    "\n",
    "X_train_vectorized_2 = tfidf_vect.transform(X_train)\n",
    "X_test_vectorized_2 = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build the classification model (classifier)\n",
    "\n",
    "LR_model_2 = LogisticRegression()\n",
    "LR_model_2.fit(X_train_vectorized_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76870748299319724"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate the classifier, i.e. calculate its classification rate\n",
    "\n",
    "LR_predictions_2 = LR_model_2.predict(X_test_vectorized_2)\n",
    "accuracy_score(y_test, LR_predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model interpretation: which vocabulary words are most significant?\n",
    "\n",
    "#Sort vocabulary words according to their max tfidf feature value over all documents\n",
    "sorted_tfidf_index = X_train_vectorized_2.max(0).toarray()[0].argsort()\n",
    "\n",
    "#Sort vocabulary words according to their LR model coefficient value\n",
    "sorted_coef2_index = LR_model_2.coef_[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words sorted by tfidf feature value:\n",
      "  Smallest:\n",
      "['chay' 'm3ak' 'you' 'love' 'sans' 'Ÿáÿ∞ÿß' 'femme' 'men' 'ŸÖŸÜ' 'ŸÉÿßŸÜ']\n",
      "\n",
      "  Largest: \n",
      "['Ÿäÿß' 'dima' 'masta' 'maquillage' 'mahlek' 'ma7lek' 'ma7leha' 'la' 'fi'\n",
      " 'est']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary words sorted by tfidf feature value:')\n",
    "print('  Smallest:\\n{}\\n'.format(tfidf_feature_names[sorted_tfidf_index[:10]]))\n",
    "print('  Largest: \\n{}'.format(tfidf_feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words sorted by LR model coefficient value:\n",
      "  Smallest Coefs:\n",
      "['3zouza' '5ayba' 'ÿπÿ≤Ÿàÿ≤ÿ©' 'masta' 'maquillage' 'ŸÅŸä' 't3awef' 'hhhhhh' 'ya'\n",
      " '3ifsha']\n",
      "\n",
      "  Largest Coefs: \n",
      "['belle' 'bayouta' 'ma7lek' 'mahlek' 'ŸÖÿ≠ŸÑÿßŸÉ' '3asla' 'thabel' 'dima'\n",
      " 'ma7leha' '3lik']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary words sorted by LR model coefficient value:')\n",
    "print('  Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('  Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
